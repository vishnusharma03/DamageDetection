#!/usr/bin/env python

from __future__ import print_function

import json
from ultralytics import YOLO
import torch
import os
import sys
import traceback
from extract import extraction, delete_files

# These are the paths to where SageMaker mounts interesting things in your container.

path = '/opt/ml/input/data/training/'

prefix = '/opt/ml/' # works on wsl, use /opt/ml/ for container

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = prefix + 'model'
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name = 'training'
training_path = os.path.join(input_path, channel_name)


# The function to execute the training.
def train():
    print('Starting the training.')
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)

        epoc = int(trainingParams.get('epochs', "10"))
        gpu = trainingParams.get('gpu', 'gpu')
        workers = int(trainingParams.get('num_workers', "8"))
        resume = trainingParams.get('resume', True)
        imgsiz = int(trainingParams.get('imgsize', "640"))
        dataPath = '/opt/ml/input/data/training/data.yaml'
        batch_size = int(trainingParams.get('batch', "8"))


        # Load a pretrained YOLO model (recommended for training)
        model = YOLO('yolov8n.pt')

        torch.cuda.empty_cache()

        # ... rest of your train function, using resolved_outPath
        if gpu == "cpu":
            device = "cpu"
            print("Training on cpu")
        else:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            print("Training on " + "cuda" if torch.cuda.is_available() else "cpu")

        # Train the model using your custom dataset for n epochs

        if resume:
            train_args = {"data": dataPath, "epochs": epoc, "imgsz": imgsiz, "project": model_path, "workers": workers,
                          "exist_ok": True, "device": device,
                          "pretrained": model_path +"/train/weights/best.pt" }  # "./model/train/weights/best.pt"
            model.train(**train_args)

        else:
            train_args = {"data": dataPath, "epochs": epoc, "imgsz": imgsiz, "project": model_path, "workers": workers,
                          "exist_ok": True, "device": device}
            model.train(**train_args)

        # Evaluate the model's performance on the validation set.
        # The validation results are logged to a W&B table.
        #model.val()

        # Save the model to the mounted folder if required.

        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

    finally:
        torch.cuda.empty_cache()
        delete_files(path)


if __name__ == '__main__':
    extraction(path)

    train()
    
    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)

